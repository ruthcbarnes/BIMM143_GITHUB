---
title: "Class 7: Machine Learning 1"
author: "Ruth Barnes: A16747659"
date: Janurary 28, 2025
format: pdf
---

Today we will explore unsupervised machine learning methods including clustering and dimensionality reduction methods

Let's start by making up some data (where we know there are clear groups) that we can use to test out different clustering methods.

We can use the `rnorm()` function to help us here:

```{r}
hist( rnorm(n=3000, mean=3) )
```

Make data `z` with two "clusters"

```{r}
x <- c( rnorm(30, mean=-3), 
        rnorm(30, mean=+3) )

z <- cbind(x= x, y= rev(x))
head(z)

plot(z)
```
How big is `z`
```{r}
nrow(z)
ncol(z)
```

## K-means clustering

The main function in "base" R for K-means clustering is called `kmeans()`

```{r}
k <- kmeans(z, centers = 2)
k
```

```{r}
attributes(k)
```

***Q. How many points lie in each cluster?***

```{r}
k$size
```

***Q. What component of our results tells us about the cluster membership (i.e. which point lies in the which cluster)?***

```{r}
k$cluster
```

***Q. Center of each cluster?***
```{r}
k$centers
```

***Q. Put this result info together and make a little "base R" plot of our clustering result.Also add the cluster center points to this plot***

```{r}
plot(z, col="blue")
```

```{r}
plot(z, col=c("blue", "red"))
```
You can color by number:

```{r}
plot(z, col=(c(1,2)))
```
Plot colored by cluster membership:

```{r}
plot(z, col=k$cluster)
points(k$centers, col="green", pch=15)
```

***Q. Run kmeans on our input `z` and define 4 clusters making the same result visualization plot as above (plot of z colored by cluster membership)***

```{r}
k4 <- kmeans(z, centers = 4)
plot(z, col=k4$cluster)
```

## Hierarchical Clustering

The main function in base R for this called `hclust()` it will take as input a distance matrix (key point is that you can't just give you "raw" data as input - you have to first calculate a distance matrix from your data).

```{r}
d <- dist(z)
hc <- hclust(d)
hc
```

```{r}
plot(hc)
abline (h=10, col="red")
```

Once I inspect the "tree" I can "cut" the tree to yield my groupings or clusters. The function to this is called `cutree()`.

```{r}
grps <- cutree(hc, h=10)
grps
```

```{r}
plot(z, col=grps)
```

# Hands on with Principal Component Analysis of UK food data (PCA)

Let's examine some silly 17-dimensional data detailing food consumption in the UK (England, Scotland, Wales, and N. Ireland). Are these countries eating habits different or similar and if so how?


## Data Import

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1) 
x
```

***Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?***

```{r}
nrow(x)
ncol(x)
dim(x)
```
## Checking your data

***Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?***

I prefer the second approach, (`x <- read.csv(url, row.names=1)`) because it is simpler, more efficient, neat, and less prone to errors. If the first code (`rownames(x) <- x[,1]; x <- x[,-1]`), x is used multiple times and thus it's error prone if other variables are named x.

## Spotting major differences and trends

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

***Q3. Changing what optional argument in the above barplot() function results in the following plot?***
 
Changing the optinal argument `beside=T` to `beside=F` in the above barplot() function results in the below plot.

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

***Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?***

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

The code `pairs(x, col=rainbow(nrow(x)), pch=16)` generates a pairwise plots of the column x, with a rainbow palette of each of the data points, as filled points. The resulting figure is a scatter plot matrix, with each plot corresponding to a pairwise relationship between two variables from the data set x. The figures help visually see distribution and correlation. If a given point lies on the diagonal for a given plot, that means the variables between the two countries are correlated.

Looking at these types of "pairwise plots" can be helpful but it does not scale well and kind of sucks! There must be a better way...

***Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?***

The main differences between N. Ireland and the other countries of the UK in terms of this data-set is that N. Ireland consumes higher amounts of carcass meat and fresh potatoes, and lower amounts of fish, vegetables, fresh fruit, and confectionery than the other countries of the UK.

## PCA to the rescue!

The main function for PCA in base R is called `prcomp()`. This function wants the transpose of our input data - i.e. the important foods in as columns and the countries as rows.

```{r}
pca <- prcomp(t(x))
summary(pca)
```

Let's see what is in our PCA result object `pca`

```{r}
plot(pca)
attributes(pca)
```

The `pca$x` result object is where we will focus first as this details how the countries are related to each other in terms of our new "axis" (a.k.a. "PCs", "eigenvectors", etc.)

```{r}
head(pca$x)
```

***Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.***

```{r}
plot(pca$x[,1], pca$x[,2], 
     xlim=c(-270,500), pch=32, 
     xlab="PC1", ylab = "PC2")
text(pca$x[,1], pca$x[,2], colnames(x))
```

***Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.***

```{r}
plot(pca$x[,1], pca$x[,2],
     pch =32,
     xlab="PC1", ylab = "PC2")
text(pca$x[,1], pca$x[,2], colnames(x), col=c("orange", "red", "blue", "darkgreen"), )
```

We can look at the so-called PC "loadings" result object to see how the original foods contribute to our new PCs (i.e. how the original variables contribute to our new better PC variables)

```{r}
pca$rotation[,1]
```

## Digging deeper (variable loadings)

Lets focus on PC1 as it accounts for > 90% of variance 

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

***Q9. Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominently and what does PC2 mainly tell us about?***

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

The two food groups that are prominently featured, were Soft Drinks and Fresh Potatoes these two variables are the main contributes to PC2. PC2 mainly tells us more specifically the variance between the countries.

